# -*- coding: utf-8 -*-
"""intro_to_pytorch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_nBe8RsHBFnUrEopbmzPLYCZKUPEzGxq

# Intro to Pytorch

Follow this notebook to create a model that can classify handwritten digits and learn a little bit about Pytorch.

## BUT BEFORE ANYTHING ELSE MAKE A COPY OF THIS NOTEBOOK OR ELSE YOU WILL LOSE YOUR CODE ONCE YOU SAVE
File > Save a Copy in Drive
"""

# Import necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader


# Define transformations for the training set
transform = transforms.Compose(
    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]
)

# Download and load the training data
trainset = torchvision.datasets.MNIST(
    root="./data", train=True, download=True, transform=transform
)
trainloader = DataLoader(trainset, batch_size=64, shuffle=True)

# Download and load the test data
testset = torchvision.datasets.MNIST(
    root="./data", train=False, download=True, transform=transform
)
testloader = DataLoader(testset, batch_size=64, shuffle=False)

# Define the neural network
model = nn.Sequential(
    nn.Flatten(),
    nn.Linear(28 * 28, 128),
    nn.ReLU(),
    nn.Linear(128, 64),
    nn.ReLU(),
    nn.Linear(64, 10),
)

"""This creates a model with an input layers of 28 * 28 = 784 nodes, then this feeds into a hidden layer of 128 nodes, then into another hidden layer of 64 nodes, then finally into an output layer with 10 nodes.

The activation funtction is ReLU, that is, $ReLU(x) = max(0, x)$, basically $ReLU(x) = x$ if $x \geq 0$ otherwise it is 0.
"""

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

"""This is setting up the optimizer that will be used to train the model parameters. The optimizer is Adam with a learning rate of 0.001. This learning rate means that the weights of the model are adjust by 0.001 whatever the training suggests. This is to stop the model from changing too much during any one training iteration."""

# Training the model
epochs = 5
for epoch in range(epochs):
    running_loss = 0.0
    for images, labels in trainloader:
        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        # Print statistics
        running_loss += loss.item()
    print(f"Epoch {epoch + 1}, Loss: {running_loss / len(trainloader)}")

print("Finished Training")

"""This trains the model 5 times in a row (i.e. 5 epochs)."""

# Testing the model
correct = 0
total = 0
with torch.no_grad():
    for images, labels in testloader:
        outputs = model(images)
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print(f"Accuracy: {100 * correct / total}%")

"""Hope this makes some sense, you are welcome to try and mess with any of the code above, maybe see what other model architecture (# of hidden layers/nuerons) works well?"""
